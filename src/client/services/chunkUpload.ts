import { stat as statAsync, open as openFile } from "fs/promises";
import { basename } from "path";
import { createHash } from "crypto";
import cliProgress from "cli-progress";
import { calculateChecksumFromFile } from "@/utils/checksum";

// åˆ†ç‰‡å¤§å° 1MB
const CHUNK_SIZE = 1 * 1024 * 1024;
// å¹¶å‘ä¸Šä¼ æ•°é‡
const CONCURRENCY = 3;
// é‡è¯•é…ç½®
const MAX_RETRIES = 3;
const INITIAL_RETRY_DELAY = 1000; // 1ç§’
const MAX_RETRY_DELAY = 10000; // 10ç§’

/**
 * å»¶è¿Ÿå‡½æ•°
 */
const delay = (ms: number) => new Promise((resolve) => setTimeout(resolve, ms));

/**
 * åˆ†ç‰‡ä¸Šä¼ æ–‡ä»¶
 * æ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œå¹¶å‘ä¸Šä¼ 
 */
export async function uploadFileChunked(
  filePath: string,
  serverUrl: string,
  token: string,
  env: string,
  shouldExtract: boolean = false
): Promise<any> {
  // è·å–æ–‡ä»¶ä¿¡æ¯
  const stats = await statAsync(filePath);
  const fileSize = stats.size;
  const fileName = basename(filePath);

  console.log(`\nğŸ“„ Uploading file (chunked): ${fileName}`);
  console.log(`ğŸš€ Uploading to ${serverUrl}...`);
  console.log(`ğŸ“¦ File size: ${(fileSize / 1024 / 1024).toFixed(2)} MB`);

  // è®¡ç®—æ–‡ä»¶æ ¡éªŒå’Œï¼ˆåŒæ—¶ä½œä¸º uploadIdï¼‰
  console.log(`ğŸ” Calculating checksum (used as uploadId)...`);
  const checksum = await calculateChecksumFromFile(filePath);
  const uploadId = checksum.substring(0, 32); // ä½¿ç”¨å‰32ä½ä½œä¸ºuploadId
  console.log(`âœ… Upload ID: ${uploadId}`);

  // è®¡ç®—åˆ†ç‰‡æ•°é‡
  const totalChunks = Math.ceil(fileSize / CHUNK_SIZE);
  console.log(
    `ğŸ“Š Total chunks: ${totalChunks} (${(CHUNK_SIZE / 1024 / 1024).toFixed(
      1
    )} MB each)`
  );

  // åˆå§‹åŒ–ä¸Šä¼ ä»»åŠ¡
  const initResponse = await fetch(`${serverUrl}/upload/init`, {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      Authorization: token,
    },
    body: JSON.stringify({
      uploadId,
      totalChunks,
      fileName,
      checksum,
      shouldExtract,
      env,
    }),
  });

  if (!initResponse.ok) {
    const error = await initResponse
      .json()
      .catch(() => ({ error: "Init failed" }));
    throw new Error((error as any).error || "Failed to initialize upload");
  }

  const initResult = (await initResponse.json()) as {
    uploadedChunks: number[];
    isResume: boolean;
  };

  // è®¡ç®—éœ€è¦ä¸Šä¼ çš„åˆ†ç‰‡
  const uploadedSet = new Set(initResult.uploadedChunks);
  const chunksToUpload = [];
  for (let i = 0; i < totalChunks; i++) {
    if (!uploadedSet.has(i)) {
      chunksToUpload.push(i);
    }
  }

  if (initResult.isResume) {
    console.log(
      `â™»ï¸  Resuming upload: ${initResult.uploadedChunks.length}/${totalChunks} chunks already uploaded`
    );
  }

  if (chunksToUpload.length === 0) {
    console.log(`âœ… All chunks already uploaded, completing...`);
  } else {
    // åˆ›å»ºè¿›åº¦æ¡
    const progressBar = new cliProgress.SingleBar({
      format: "ğŸ“¤ [{bar}] {percentage}% | {value}/{total} chunks | {speed}",
      barCompleteChar: "\u2588",
      barIncompleteChar: "\u2591",
      hideCursor: true,
    });

    const startTime = Date.now();
    let completedChunks = initResult.uploadedChunks.length;
    progressBar.start(totalChunks, completedChunks, { speed: "0 chunks/s" });

    // æ‰“å¼€æ–‡ä»¶å¥æŸ„
    const fileHandle = await openFile(filePath, "r");

    // ä¸Šä¼ å•ä¸ªåˆ†ç‰‡ï¼ˆä¸å«é‡è¯•ï¼‰
    const uploadChunkOnce = async (
      chunkIndex: number,
      buffer: Buffer
    ): Promise<void> => {
      // è®¡ç®—åˆ†ç‰‡ MD5
      const chunkMd5 = createHash("md5").update(buffer).digest("hex");

      const response = await fetch(
        `${serverUrl}/upload/chunk?uploadId=${uploadId}&chunkIndex=${chunkIndex}&env=${env}`,
        {
          method: "POST",
          headers: {
            "Content-Type": "application/octet-stream",
            Authorization: token,
            "X-Chunk-MD5": chunkMd5,
          },
          body: buffer,
        }
      );

      if (!response.ok) {
        const error = await response
          .json()
          .catch(() => ({ error: "Chunk upload failed" }));
        throw new Error(`Chunk ${chunkIndex} failed: ${(error as any).error}`);
      }
    };

    // å¸¦é‡è¯•çš„åˆ†ç‰‡ä¸Šä¼ 
    const uploadChunkWithRetry = async (chunkIndex: number): Promise<void> => {
      const offset = chunkIndex * CHUNK_SIZE;
      const size = Math.min(CHUNK_SIZE, fileSize - offset);
      const buffer = Buffer.alloc(size);

      await fileHandle.read(buffer, 0, size, offset);

      let lastError: Error | null = null;

      for (let attempt = 0; attempt <= MAX_RETRIES; attempt++) {
        try {
          await uploadChunkOnce(chunkIndex, buffer);

          // æˆåŠŸï¼Œæ›´æ–°è¿›åº¦
          completedChunks++;
          const elapsed = (Date.now() - startTime) / 1000;
          const speed =
            (completedChunks - initResult.uploadedChunks.length) / elapsed;
          progressBar.update(completedChunks, {
            speed: `${speed.toFixed(1)} chunks/s`,
          });
          return;
        } catch (error: any) {
          lastError = error;

          if (attempt < MAX_RETRIES) {
            // è®¡ç®—é€€é¿å»¶è¿Ÿï¼ˆæŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨ï¼‰
            const baseDelay = Math.min(
              INITIAL_RETRY_DELAY * Math.pow(2, attempt),
              MAX_RETRY_DELAY
            );
            const jitter = Math.random() * 500;
            const retryDelay = baseDelay + jitter;

            console.log(
              `\nâš ï¸  Chunk ${chunkIndex} failed, retrying in ${(
                retryDelay / 1000
              ).toFixed(1)}s... (${attempt + 1}/${MAX_RETRIES})`
            );
            await delay(retryDelay);
          }
        }
      }

      // æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
      throw (
        lastError ||
        new Error(`Chunk ${chunkIndex} failed after ${MAX_RETRIES} retries`)
      );
    };

    // å¹¶å‘æ‰§è¡Œåˆ†ç‰‡ä¸Šä¼ 
    const queue = [...chunksToUpload];
    const workers: Promise<void>[] = [];

    const worker = async () => {
      while (queue.length > 0) {
        const chunkIndex = queue.shift();
        if (chunkIndex !== undefined) {
          await uploadChunkWithRetry(chunkIndex);
        }
      }
    };

    // å¯åŠ¨å¹¶å‘ workers
    for (let i = 0; i < Math.min(CONCURRENCY, chunksToUpload.length); i++) {
      workers.push(worker());
    }

    await Promise.all(workers);
    await fileHandle.close();

    progressBar.stop();
    console.log(`âœ… All chunks uploaded`);
  }

  // å®Œæˆä¸Šä¼ 
  console.log(`âš™ï¸  Processing file on server...`);
  const completeResponse = await fetch(`${serverUrl}/upload/complete`, {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      Authorization: token,
    },
    body: JSON.stringify({
      uploadId,
      fileName,
      checksum,
      shouldExtract,
      env,
    }),
  });

  if (!completeResponse.ok) {
    const error = await completeResponse
      .json()
      .catch(() => ({ error: "Complete failed" }));
    throw new Error(
      (error as any).error ||
        (error as any).details ||
        "Failed to complete upload"
    );
  }

  const result = await completeResponse.json();
  console.log(`âœ… Upload completed successfully!`);
  return result;
}

/**
 * åˆ†ç‰‡ä¸Šä¼ ç›®å½•ï¼ˆå…ˆå‹ç¼©ï¼Œå†åˆ†ç‰‡ä¸Šä¼ ï¼‰
 */
export async function uploadDirectoryChunked(
  dirPath: string,
  serverUrl: string,
  token: string,
  env: string,
  excludePatterns: string[] = []
): Promise<any> {
  const { withTempZip } = await import("./archive");

  return withTempZip(dirPath, env, excludePatterns, async (tempZipPath) => {
    return uploadFileChunked(
      tempZipPath,
      serverUrl,
      token,
      env,
      true // ç›®å½•å‹ç¼©åéœ€è¦è§£å‹
    );
  });
}
