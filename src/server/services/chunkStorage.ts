import { existsSync, mkdirSync, readdirSync, statSync, rmSync } from "fs";
import { join } from "path";
import { tmpdir } from "os";
import { mkdir, writeFile, readFile, readdir, rm, stat } from "fs/promises";

// é»˜è®¤é…ç½® - ä½¿ç”¨ç³»ç»Ÿä¸´æ—¶ç›®å½•ï¼Œæ”¯æŒè·¨å¹³å°
const CHUNK_DIR = process.env.CHUNK_UPLOAD_DIR || join(tmpdir(), "fde-chunks");
const EXPIRY_MS = 24 * 60 * 60 * 1000; // 24å°æ—¶

/**
 * ä¸Šä¼ ä»»åŠ¡å…ƒæ•°æ®
 */
interface UploadMetadata {
  uploadId: string;
  totalChunks: number;
  uploadedChunks: number[];
  fileName?: string;
  checksum?: string;
  env: string;
  shouldExtract: boolean;
  createdAt: number;
  updatedAt: number;
}

/**
 * ç¡®ä¿åˆ†ç‰‡å­˜å‚¨ç›®å½•å­˜åœ¨
 */
function ensureChunkDir(): void {
  if (!existsSync(CHUNK_DIR)) {
    mkdirSync(CHUNK_DIR, { recursive: true });
  }
}

/**
 * è·å–ä¸Šä¼ ä»»åŠ¡ç›®å½•
 */
function getUploadDir(uploadId: string): string {
  return join(CHUNK_DIR, uploadId);
}

/**
 * è·å–åˆ†ç‰‡æ–‡ä»¶è·¯å¾„
 */
function getChunkPath(uploadId: string, chunkIndex: number): string {
  return join(
    getUploadDir(uploadId),
    `chunk_${chunkIndex.toString().padStart(6, "0")}`
  );
}

/**
 * è·å–å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
 */
function getMetadataPath(uploadId: string): string {
  return join(getUploadDir(uploadId), "metadata.json");
}

/**
 * åˆå§‹åŒ–æˆ–è·å–ä¸Šä¼ ä»»åŠ¡
 */
export async function initUpload(
  uploadId: string,
  totalChunks: number,
  env: string,
  shouldExtract: boolean = false
): Promise<UploadMetadata> {
  ensureChunkDir();
  const uploadDir = getUploadDir(uploadId);
  const metadataPath = getMetadataPath(uploadId);

  // å¦‚æœå·²å­˜åœ¨ï¼Œè¯»å–ç°æœ‰å…ƒæ•°æ®
  if (existsSync(metadataPath)) {
    const existing = JSON.parse(
      await readFile(metadataPath, "utf-8")
    ) as UploadMetadata;
    return existing;
  }

  // åˆ›å»ºæ–°ä¸Šä¼ ä»»åŠ¡
  await mkdir(uploadDir, { recursive: true });

  const metadata: UploadMetadata = {
    uploadId,
    totalChunks,
    uploadedChunks: [],
    env,
    shouldExtract,
    createdAt: Date.now(),
    updatedAt: Date.now(),
  };

  await writeFile(metadataPath, JSON.stringify(metadata, null, 2));
  return metadata;
}

/**
 * è·å–ä¸Šä¼ çŠ¶æ€
 */
export async function getUploadStatus(uploadId: string): Promise<{
  exists: boolean;
  uploadedChunks: number[];
  totalChunks?: number;
}> {
  const metadataPath = getMetadataPath(uploadId);

  if (!existsSync(metadataPath)) {
    return { exists: false, uploadedChunks: [] };
  }

  try {
    const metadata = JSON.parse(
      await readFile(metadataPath, "utf-8")
    ) as UploadMetadata;
    return {
      exists: true,
      uploadedChunks: metadata.uploadedChunks,
      totalChunks: metadata.totalChunks,
    };
  } catch {
    return { exists: false, uploadedChunks: [] };
  }
}

/**
 * ä¿å­˜åˆ†ç‰‡
 */
export async function saveChunk(
  uploadId: string,
  chunkIndex: number,
  data: Buffer
): Promise<{ success: boolean; chunkIndex: number }> {
  const uploadDir = getUploadDir(uploadId);
  const chunkPath = getChunkPath(uploadId, chunkIndex);
  const metadataPath = getMetadataPath(uploadId);

  // ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
  if (!existsSync(uploadDir)) {
    await mkdir(uploadDir, { recursive: true });
  }

  // ä¿å­˜åˆ†ç‰‡
  await writeFile(chunkPath, data);

  // æ›´æ–°å…ƒæ•°æ®
  if (existsSync(metadataPath)) {
    const metadata = JSON.parse(
      await readFile(metadataPath, "utf-8")
    ) as UploadMetadata;
    if (!metadata.uploadedChunks.includes(chunkIndex)) {
      metadata.uploadedChunks.push(chunkIndex);
      metadata.uploadedChunks.sort((a, b) => a - b);
    }
    metadata.updatedAt = Date.now();
    await writeFile(metadataPath, JSON.stringify(metadata, null, 2));
  }

  return { success: true, chunkIndex };
}

/**
 * åˆå¹¶æ‰€æœ‰åˆ†ç‰‡
 */
export async function mergeChunks(uploadId: string): Promise<Buffer> {
  const metadataPath = getMetadataPath(uploadId);

  if (!existsSync(metadataPath)) {
    throw new Error(`Upload ${uploadId} not found`);
  }

  const metadata = JSON.parse(
    await readFile(metadataPath, "utf-8")
  ) as UploadMetadata;

  // æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åˆ†ç‰‡éƒ½å·²ä¸Šä¼ 
  if (metadata.uploadedChunks.length !== metadata.totalChunks) {
    throw new Error(
      `Incomplete upload: ${metadata.uploadedChunks.length}/${metadata.totalChunks} chunks received`
    );
  }

  // æŒ‰é¡ºåºè¯»å–å¹¶åˆå¹¶åˆ†ç‰‡
  const chunks: Buffer[] = [];
  for (let i = 0; i < metadata.totalChunks; i++) {
    const chunkPath = getChunkPath(uploadId, i);
    if (!existsSync(chunkPath)) {
      throw new Error(`Missing chunk ${i}`);
    }
    chunks.push(await readFile(chunkPath));
  }

  return Buffer.concat(chunks);
}

/**
 * åˆ é™¤ä¸Šä¼ ä»»åŠ¡ï¼ˆæˆåŠŸæˆ–å¤±è´¥åè°ƒç”¨ï¼‰
 */
export async function deleteUpload(uploadId: string): Promise<void> {
  const uploadDir = getUploadDir(uploadId);
  if (existsSync(uploadDir)) {
    await rm(uploadDir, { recursive: true, force: true });
  }
}

/**
 * æ¸…ç†è¿‡æœŸä¸Šä¼ ä»»åŠ¡
 */
export async function cleanupExpiredUploads(): Promise<number> {
  ensureChunkDir();
  let cleaned = 0;

  try {
    const dirs = await readdir(CHUNK_DIR);
    const now = Date.now();

    for (const dir of dirs) {
      const metadataPath = join(CHUNK_DIR, dir, "metadata.json");

      try {
        if (existsSync(metadataPath)) {
          const metadata = JSON.parse(
            await readFile(metadataPath, "utf-8")
          ) as UploadMetadata;
          if (now - metadata.updatedAt > EXPIRY_MS) {
            await rm(join(CHUNK_DIR, dir), { recursive: true, force: true });
            console.log(`ğŸ—‘ï¸ Cleaned up expired upload: ${dir}`);
            cleaned++;
          }
        } else {
          // æ²¡æœ‰å…ƒæ•°æ®æ–‡ä»¶çš„ç›®å½•ï¼Œæ£€æŸ¥åˆ›å»ºæ—¶é—´
          const dirStat = await stat(join(CHUNK_DIR, dir));
          if (now - dirStat.mtimeMs > EXPIRY_MS) {
            await rm(join(CHUNK_DIR, dir), { recursive: true, force: true });
            console.log(`ğŸ—‘ï¸ Cleaned up orphan upload dir: ${dir}`);
            cleaned++;
          }
        }
      } catch (e) {
        // å¿½ç•¥å•ä¸ªç›®å½•çš„é”™è¯¯
      }
    }
  } catch (e) {
    console.error("Failed to clean up expired uploads:", e);
  }

  return cleaned;
}

/**
 * å¯åŠ¨å®šæ—¶æ¸…ç†ä»»åŠ¡
 */
export function startCleanupScheduler(
  intervalMs: number = 60 * 60 * 1000
): NodeJS.Timeout {
  console.log(
    `ğŸ§¹ Chunk cleanup scheduler started (every ${
      intervalMs / 1000 / 60
    } minutes)`
  );

  // å¯åŠ¨æ—¶å…ˆæ¸…ç†ä¸€æ¬¡
  cleanupExpiredUploads().then((count) => {
    if (count > 0) {
      console.log(`ğŸ§¹ Initial cleanup: removed ${count} expired uploads`);
    }
  });

  return setInterval(() => {
    cleanupExpiredUploads().then((count) => {
      if (count > 0) {
        console.log(`ğŸ§¹ Scheduled cleanup: removed ${count} expired uploads`);
      }
    });
  }, intervalMs);
}
